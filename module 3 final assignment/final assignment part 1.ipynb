{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 771948_A23_T3A - Group Work Assignment #\n",
    "# Part 1 - Numerical and categorical feature classification problem #\n",
    "## Assignment by Chris Mintz 202369825 and Antonia Agunbiade 202375309 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Load and preprocess the dataset for the classification problem (handle missing data, convert categorical features to numerical features) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>var3</th>\n",
       "      <th>var4</th>\n",
       "      <th>var5</th>\n",
       "      <th>var6</th>\n",
       "      <th>var7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>509.18</td>\n",
       "      <td>417.681</td>\n",
       "      <td>Micronesia</td>\n",
       "      <td>138.0</td>\n",
       "      <td>393.00</td>\n",
       "      <td>no</td>\n",
       "      <td>2019-07-20 13:21:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>446.06</td>\n",
       "      <td>666.182</td>\n",
       "      <td>Dominica</td>\n",
       "      <td>81.0</td>\n",
       "      <td>352.05</td>\n",
       "      <td>yes</td>\n",
       "      <td>2019-04-04 21:30:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>235.50</td>\n",
       "      <td>398.097</td>\n",
       "      <td>Isle of Man</td>\n",
       "      <td>90.0</td>\n",
       "      <td>339.00</td>\n",
       "      <td>no</td>\n",
       "      <td>2019-03-03 02:59:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>306.02</td>\n",
       "      <td>518.163</td>\n",
       "      <td>Turkmenistan</td>\n",
       "      <td>102.0</td>\n",
       "      <td>439.25</td>\n",
       "      <td>yes</td>\n",
       "      <td>2019-03-19 08:00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>453.08</td>\n",
       "      <td>600.156</td>\n",
       "      <td>Cameroon</td>\n",
       "      <td>105.0</td>\n",
       "      <td>422.95</td>\n",
       "      <td>no</td>\n",
       "      <td>2019-03-18 13:22:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>211.72</td>\n",
       "      <td>506.716</td>\n",
       "      <td>Liechtenstein</td>\n",
       "      <td>111.0</td>\n",
       "      <td>310.60</td>\n",
       "      <td>no</td>\n",
       "      <td>2019-03-18 13:00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>401.42</td>\n",
       "      <td>627.294</td>\n",
       "      <td>French Guiana</td>\n",
       "      <td>78.0</td>\n",
       "      <td>390.05</td>\n",
       "      <td>no</td>\n",
       "      <td>2019-03-28 02:29:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>498.90</td>\n",
       "      <td>525.207</td>\n",
       "      <td>Barbados</td>\n",
       "      <td>129.0</td>\n",
       "      <td>408.75</td>\n",
       "      <td>yes</td>\n",
       "      <td>2019-06-07 05:41:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>257.90</td>\n",
       "      <td>651.209</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>147.0</td>\n",
       "      <td>280.20</td>\n",
       "      <td>no</td>\n",
       "      <td>2019-02-07 08:02:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>283.04</td>\n",
       "      <td>467.801</td>\n",
       "      <td>Chad</td>\n",
       "      <td>69.0</td>\n",
       "      <td>272.35</td>\n",
       "      <td>yes</td>\n",
       "      <td>2019-03-26 19:37:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>333.72</td>\n",
       "      <td>188.193</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>90.0</td>\n",
       "      <td>289.30</td>\n",
       "      <td>yes</td>\n",
       "      <td>2019-07-08 17:14:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>440.16</td>\n",
       "      <td>703.248</td>\n",
       "      <td>Greenland</td>\n",
       "      <td>87.0</td>\n",
       "      <td>341.25</td>\n",
       "      <td>yes</td>\n",
       "      <td>2019-02-15 16:18:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>450.46</td>\n",
       "      <td>654.968</td>\n",
       "      <td>Dominica</td>\n",
       "      <td>114.0</td>\n",
       "      <td>412.90</td>\n",
       "      <td>no</td>\n",
       "      <td>2019-03-11 14:50:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>451.52</td>\n",
       "      <td>634.303</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>75.0</td>\n",
       "      <td>338.55</td>\n",
       "      <td>no</td>\n",
       "      <td>2019-04-28 02:55:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>230.74</td>\n",
       "      <td>575.947</td>\n",
       "      <td>Equatorial Guinea</td>\n",
       "      <td>132.0</td>\n",
       "      <td>260.85</td>\n",
       "      <td>no</td>\n",
       "      <td>2019-02-24 06:17:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>461.86</td>\n",
       "      <td>612.276</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>93.0</td>\n",
       "      <td>397.85</td>\n",
       "      <td>yes</td>\n",
       "      <td>2019-05-17 04:27:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>210.00</td>\n",
       "      <td>441.743</td>\n",
       "      <td>Austria</td>\n",
       "      <td>132.0</td>\n",
       "      <td>310.30</td>\n",
       "      <td>yes</td>\n",
       "      <td>2019-04-12 14:01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>374.06</td>\n",
       "      <td>772.204</td>\n",
       "      <td>Equatorial Guinea</td>\n",
       "      <td>102.0</td>\n",
       "      <td>343.40</td>\n",
       "      <td>no</td>\n",
       "      <td>2019-07-15 09:42:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>231.82</td>\n",
       "      <td>498.228</td>\n",
       "      <td>Brunei Darussalam</td>\n",
       "      <td>99.0</td>\n",
       "      <td>257.90</td>\n",
       "      <td>yes</td>\n",
       "      <td>2019-04-24 13:46:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>398.58</td>\n",
       "      <td>541.062</td>\n",
       "      <td>Luxembourg</td>\n",
       "      <td>69.0</td>\n",
       "      <td>391.20</td>\n",
       "      <td>yes</td>\n",
       "      <td>2019-04-04 11:39:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    target    var1     var2               var3   var4    var5 var6  \\\n",
       "0        0  509.18  417.681         Micronesia  138.0  393.00   no   \n",
       "1        0  446.06  666.182           Dominica   81.0  352.05  yes   \n",
       "2        1  235.50  398.097        Isle of Man   90.0  339.00   no   \n",
       "3        0  306.02  518.163       Turkmenistan  102.0  439.25  yes   \n",
       "4        0  453.08  600.156           Cameroon  105.0  422.95   no   \n",
       "5        1  211.72  506.716      Liechtenstein  111.0  310.60   no   \n",
       "6        0  401.42  627.294      French Guiana   78.0  390.05   no   \n",
       "7        0  498.90  525.207           Barbados  129.0  408.75  yes   \n",
       "8        1  257.90  651.209        Netherlands  147.0  280.20   no   \n",
       "9        1  283.04  467.801               Chad   69.0  272.35  yes   \n",
       "10       1  333.72  188.193            Algeria   90.0  289.30  yes   \n",
       "11       0  440.16  703.248          Greenland   87.0  341.25  yes   \n",
       "12       0  450.46  654.968           Dominica  114.0  412.90   no   \n",
       "13       0  451.52  634.303            Armenia   75.0  338.55   no   \n",
       "14       1  230.74  575.947  Equatorial Guinea  132.0  260.85   no   \n",
       "15       0  461.86  612.276              Egypt   93.0  397.85  yes   \n",
       "16       1  210.00  441.743            Austria  132.0  310.30  yes   \n",
       "17       0  374.06  772.204  Equatorial Guinea  102.0  343.40   no   \n",
       "18       1  231.82  498.228  Brunei Darussalam   99.0  257.90  yes   \n",
       "19       0  398.58  541.062         Luxembourg   69.0  391.20  yes   \n",
       "\n",
       "                   var7  \n",
       "0   2019-07-20 13:21:37  \n",
       "1   2019-04-04 21:30:46  \n",
       "2   2019-03-03 02:59:37  \n",
       "3   2019-03-19 08:00:58  \n",
       "4   2019-03-18 13:22:35  \n",
       "5   2019-03-18 13:00:12  \n",
       "6   2019-03-28 02:29:19  \n",
       "7   2019-06-07 05:41:16  \n",
       "8   2019-02-07 08:02:31  \n",
       "9   2019-03-26 19:37:46  \n",
       "10  2019-07-08 17:14:01  \n",
       "11  2019-02-15 16:18:49  \n",
       "12  2019-03-11 14:50:56  \n",
       "13  2019-04-28 02:55:10  \n",
       "14  2019-02-24 06:17:18  \n",
       "15  2019-05-17 04:27:31  \n",
       "16  2019-04-12 14:01:08  \n",
       "17  2019-07-15 09:42:19  \n",
       "18  2019-04-24 13:46:10  \n",
       "19  2019-04-04 11:39:51  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "df = pd.read_excel('dataset1.xlsx')\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 925 entries, 0 to 924\n",
      "Data columns (total 8 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   target  925 non-null    int64  \n",
      " 1   var1    925 non-null    float64\n",
      " 2   var2    925 non-null    float64\n",
      " 3   var3    925 non-null    object \n",
      " 4   var4    325 non-null    float64\n",
      " 5   var5    925 non-null    float64\n",
      " 6   var6    925 non-null    object \n",
      " 7   var7    925 non-null    object \n",
      "dtypes: float64(4), int64(1), object(3)\n",
      "memory usage: 57.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>var4</th>\n",
       "      <th>var5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>925.000000</td>\n",
       "      <td>925.000000</td>\n",
       "      <td>925.000000</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>925.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.496216</td>\n",
       "      <td>360.116562</td>\n",
       "      <td>548.390134</td>\n",
       "      <td>108.452308</td>\n",
       "      <td>325.393946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500256</td>\n",
       "      <td>87.866662</td>\n",
       "      <td>135.221460</td>\n",
       "      <td>26.325744</td>\n",
       "      <td>78.862779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>209.560000</td>\n",
       "      <td>139.965000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>163.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>278.640000</td>\n",
       "      <td>467.373000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>257.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>367.640000</td>\n",
       "      <td>569.841000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>342.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>437.600000</td>\n",
       "      <td>652.278000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>393.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>539.920000</td>\n",
       "      <td>794.848000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>457.150000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           target        var1        var2        var4        var5\n",
       "count  925.000000  925.000000  925.000000  325.000000  925.000000\n",
       "mean     0.496216  360.116562  548.390134  108.452308  325.393946\n",
       "std      0.500256   87.866662  135.221460   26.325744   78.862779\n",
       "min      0.000000  209.560000  139.965000   57.000000  163.000000\n",
       "25%      0.000000  278.640000  467.373000   87.000000  257.900000\n",
       "50%      0.000000  367.640000  569.841000  105.000000  342.350000\n",
       "75%      1.000000  437.600000  652.278000  126.000000  393.000000\n",
       "max      1.000000  539.920000  794.848000  180.000000  457.150000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have a look at the data types\n",
    "df.info()\n",
    "\n",
    "# quick look at the data\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='yellow'>About this dataset</font> ###\n",
    "#### var1, var2, var3, var4, var5, var6, var7 columns are features ####\n",
    "#### var1, var2, var4, var5 are numerical values ####\n",
    "#### var3, var6 columns are categorical values #### \n",
    "#### var 7 is a datetime ####\n",
    "#### target column is the label ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have null values in the data so lets enumerate them ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target      0\n",
       "var1        0\n",
       "var2        0\n",
       "var3        0\n",
       "var4      600\n",
       "var5        0\n",
       "var6        0\n",
       "var7        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the date to proper datetime. Using coerce because there are errors in the dates\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-28 12:31:57\n",
      "2019-02-28 18:06:21\n",
      "2019-02-28 11:00:06\n",
      "2019-02-28 23:56:06\n",
      "2019-02-28 19:26:35\n"
     ]
    }
   ],
   "source": [
    "# some investigation shows 5 cells with an illegal datetime in them. Specifically, 5 cells have a date of 2019-02-29 which is not a valid date\n",
    "def fix_not_leap_year(bad_date):\n",
    "    if '2019-02-29' in str(bad_date):\n",
    "        bad_date = str(bad_date).replace('2019-02-29', '2019-02-28')\n",
    "        print(bad_date)\n",
    "        return bad_date\n",
    "    else:\n",
    "        return bad_date\n",
    "\n",
    "df['var7'] = df['var7'].apply(fix_not_leap_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the the dataset is not a linear regression problem, we'll convert the datetime to features with a custom transformer\n",
    "# this will allow us to extract the day, month, year and time as separate features.\n",
    "# define a custom transformer function to extract datetime features\n",
    "def extract_datetime_features(dates):\n",
    "    dates = pd.to_datetime(dates, infer_datetime_format=True, errors='coerce')\n",
    "    return pd.DataFrame({\n",
    "        'year': dates.dt.year,\n",
    "        'month': dates.dt.month,\n",
    "        'day': dates.dt.day,\n",
    "        'dayofweek': dates.dt.dayofweek,\n",
    "        'hour': dates.dt.hour\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     year  month  day  dayofweek  hour\n",
      "0    2019      7   20          5    13\n",
      "1    2019      4    4          3    21\n",
      "2    2019      3    3          6     2\n",
      "3    2019      3   19          1     8\n",
      "4    2019      3   18          0    13\n",
      "..    ...    ...  ...        ...   ...\n",
      "920  2019      1   22          1    12\n",
      "921  2019      2    1          4    14\n",
      "922  2019      6   13          3    18\n",
      "923  2019      5   27          0     6\n",
      "924  2019      6   18          1    17\n",
      "\n",
      "[925 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skyma\\AppData\\Local\\Temp\\ipykernel_10692\\1019460975.py:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dates = pd.to_datetime(dates, infer_datetime_format=True, errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# test the datetime transformer function\n",
    "date_df = extract_datetime_features(df['var7'])\n",
    "print(date_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     target    var1     var2          var3   var4    var5 var6  \\\n",
      "0         0  509.18  417.681    Micronesia  138.0  393.00   no   \n",
      "1         0  446.06  666.182      Dominica   81.0  352.05  yes   \n",
      "2         1  235.50  398.097   Isle of Man   90.0  339.00   no   \n",
      "3         0  306.02  518.163  Turkmenistan  102.0  439.25  yes   \n",
      "4         0  453.08  600.156      Cameroon  105.0  422.95   no   \n",
      "..      ...     ...      ...           ...    ...     ...  ...   \n",
      "920       0  422.34  547.259       Belarus    NaN  350.45  yes   \n",
      "921       1  342.62  473.919         Japan    NaN  200.85   no   \n",
      "922       1  265.10  538.170  Saint Martin    NaN  208.35  yes   \n",
      "923       0  397.12  622.386          Chad    NaN  433.45  yes   \n",
      "924       1  242.10  420.429       Albania    NaN  369.20  yes   \n",
      "\n",
      "                    var7  year  month  day  dayofweek  hour  \n",
      "0    2019-07-20 13:21:37  2019      7   20          5    13  \n",
      "1    2019-04-04 21:30:46  2019      4    4          3    21  \n",
      "2    2019-03-03 02:59:37  2019      3    3          6     2  \n",
      "3    2019-03-19 08:00:58  2019      3   19          1     8  \n",
      "4    2019-03-18 13:22:35  2019      3   18          0    13  \n",
      "..                   ...   ...    ...  ...        ...   ...  \n",
      "920  2019-01-22 12:58:14  2019      1   22          1    12  \n",
      "921  2019-02-01 14:37:34  2019      2    1          4    14  \n",
      "922  2019-06-13 18:50:00  2019      6   13          3    18  \n",
      "923  2019-05-27 06:19:27  2019      5   27          0     6  \n",
      "924  2019-06-18 17:23:26  2019      6   18          1    17  \n",
      "\n",
      "[925 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# DOCS When trying to deal with the datetime, we decided to convert the datetime to numerical features before putting it into the transform/fit process.\n",
    "# This is because this is not a linear regression problem but a classification problem and we want to extract the day, month, year and time as separate features.\n",
    "df_combined = pd.concat([df, date_df], axis=1)\n",
    "print(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are too many NA in var4 to simply remove those rows so we will try to impute the missing values #\n",
    "# We will have to watch the weighting of var4 feature as it will be heavily weighted to the interpolated values #\n",
    "\n",
    "# TO DO: I don't like the interpolation of the var 4 feature. Hoping to use an algorithm that supports null data or find a regression pattern to better fit OR it's possible this is not used as feature data at all.\n",
    "\n",
    "# Data normalization #\n",
    "numerical_features = ['var1', 'var2', 'var4', 'var5', 'year', 'month', 'day', 'dayofweek', 'hour']\n",
    "categorical_features = ['var3', 'var6']\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_features),\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_features)\n",
    "    ])\n",
    "\n",
    "# the result of this will be a Compressed Sparse Row (CSR) matrix which works with XGBoost and scikit-learn.\n",
    "df_transformed = preprocessor.fit_transform(df_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Build a classifier for the classification problem using one of the specified models (logistic regression, decision trees, random forests, or artificial neural networks) #\n",
    "\n",
    "### \n",
    "<b>Refereces</b>\n",
    "\n",
    "DataCamp. (2023). Learn XGBoost in Python: A Step-by-Step Tutorial. Available at: https://www.datacamp.com/tutorial/xgboost-in-python [Accessed 19 Aug. 2024].\n",
    "\n",
    "XGBoost Contributors. (2024). XGBoost Parameters â€” xgboost 2.1.1 documentation. Available at: https://xgboost.readthedocs.io/en/stable/parameter.html [Accessed 19 Aug. 2024].\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to use XGBoost to train a decision tree model\n",
    "# we will use the transformed data from the preprocessor as the input data\n",
    "# target variable is the 'target' column from original df dataset\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# setup our features and labels\n",
    "X = df_transformed\n",
    "y = df['target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# We're going to use the DMatrix data structure from XGBoost. This is an optimized data structure that works with XGBoost and is optimized for memory and speed\n",
    "# Create the regression matrices in DMatrix format\n",
    "dtrain_reg = xgb.DMatrix(X_train, y_train)\n",
    "dtest_reg = xgb.DMatrix(X_test, y_test)\n",
    "\n",
    "\n",
    "# Now set parameters for XGBoost\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  \n",
    "    'max_depth': 6,\n",
    "    'eta': 0.3,\n",
    "    'eval_metric': 'rmse',\n",
    "    'tree_method': 'hist'\n",
    "}\n",
    "\n",
    "# We'll look at the ideal number of rounds in hyperparameter tuning but for now let's use 100 rounds\n",
    "n = 100\n",
    "model = xgb.train(\n",
    "    params, \n",
    "    dtrain_reg, \n",
    "    n\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Fine tune the selected model using appropriate techniques like hyperparameter tuning, cross-validation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of the base model:  0.260\n"
     ]
    }
   ],
   "source": [
    "# let's look at the model's performance and see if any hyperparameter tuning is needed\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "predictions = model.predict(dtest_reg)\n",
    "# compare the predictions to the actual values\n",
    "rmse = root_mean_squared_error(y_test, predictions)\n",
    "print(f'RMSE of the base model: {rmse: .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0          0.359950        0.001390        0.373904       0.002923\n",
      "1          0.261928        0.000928        0.293481       0.006864\n",
      "2          0.194283        0.002384        0.246405       0.012310\n",
      "3          0.146789        0.002882        0.220290       0.015217\n",
      "4          0.113376        0.003406        0.207328       0.018781\n",
      "5          0.090349        0.002964        0.200462       0.019948\n",
      "6          0.073828        0.002580        0.196181       0.021589\n",
      "7          0.062180        0.002946        0.193530       0.022102\n",
      "8          0.052818        0.003235        0.192888       0.023142\n",
      "9          0.045744        0.002899        0.192215       0.023588\n",
      "10         0.040799        0.003040        0.191935       0.023863\n",
      "11         0.037110        0.003348        0.191977       0.024354\n",
      "12         0.034093        0.003328        0.192074       0.024766\n",
      "13         0.031694        0.003509        0.191736       0.025091\n"
     ]
    }
   ],
   "source": [
    "# looking to improve on the RMSE by using validation sets\n",
    "# XGBoost has a built-in cross-validation function that we can use to evaluate the model\n",
    "# using early_stopping_rounds we can stop the training if the model stops improving\n",
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    dtrain_reg,\n",
    "    num_boost_round=200,\n",
    "    seed=0,\n",
    "    nfold=5,\n",
    "    metrics={'rmse'},\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE:  0.192\n"
     ]
    }
   ],
   "source": [
    "# pull out the best RMSE from the cv_results\n",
    "best_rmse = cv_results['test-rmse-mean'].min()\n",
    "print(f'Best tested RMSE: {best_rmse: .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate: 0.1\n",
      "Best max depth: 9\n",
      "Best min child weight: 5\n",
      "Best number of estimators: 100\n",
      "Best gamma: 0.1\n",
      "Best gamma: 0.5\n",
      "Best subsample: 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\skyma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py:160: UserWarning: [12:45:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"col_sample_bytree\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Now use a hyperparameter grid to test different hyperparameters and effectiveness\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.3, 0.5, 1],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'min_child_weight': [1, 2, 3, 4, 5],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3],\n",
    "    'subsample': [0.5, 0.7, 1]\n",
    "}\n",
    "\n",
    "param_model = xgb.XGBRegressor()\n",
    "\n",
    "param_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(param_model, param_grid, cv=param_cv, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f'Best learning rate: {grid_search.best_params_[\"learning_rate\"]}')\n",
    "print(f'Best max depth: {grid_search.best_params_[\"max_depth\"]}')\n",
    "print(f'Best min child weight: {grid_search.best_params_[\"min_child_weight\"]}')\n",
    "print(f'Best number of estimators: {grid_search.best_params_[\"n_estimators\"]}')\n",
    "print(f'Best gamma: {grid_search.best_params_[\"gamma\"]}')\n",
    "print(f'Best subsample: {grid_search.best_params_[\"subsample\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.38635\tvalidation-rmse:0.39255\n",
      "[1]\ttrain-rmse:0.30833\tvalidation-rmse:0.32819\n",
      "[2]\ttrain-rmse:0.26084\tvalidation-rmse:0.28579\n",
      "[3]\ttrain-rmse:0.22654\tvalidation-rmse:0.26239\n",
      "[4]\ttrain-rmse:0.20616\tvalidation-rmse:0.24722\n",
      "[5]\ttrain-rmse:0.19337\tvalidation-rmse:0.24172\n",
      "[6]\ttrain-rmse:0.18684\tvalidation-rmse:0.23680\n",
      "[7]\ttrain-rmse:0.18210\tvalidation-rmse:0.23460\n",
      "[8]\ttrain-rmse:0.17725\tvalidation-rmse:0.23058\n",
      "[9]\ttrain-rmse:0.17319\tvalidation-rmse:0.23193\n",
      "[10]\ttrain-rmse:0.17026\tvalidation-rmse:0.22581\n",
      "[11]\ttrain-rmse:0.16837\tvalidation-rmse:0.22417\n",
      "[12]\ttrain-rmse:0.16773\tvalidation-rmse:0.21873\n",
      "[13]\ttrain-rmse:0.16750\tvalidation-rmse:0.22066\n",
      "[14]\ttrain-rmse:0.16747\tvalidation-rmse:0.22155\n",
      "[15]\ttrain-rmse:0.16667\tvalidation-rmse:0.21950\n",
      "[16]\ttrain-rmse:0.16683\tvalidation-rmse:0.22186\n",
      "[17]\ttrain-rmse:0.16640\tvalidation-rmse:0.22166\n",
      "[18]\ttrain-rmse:0.16551\tvalidation-rmse:0.21709\n",
      "[19]\ttrain-rmse:0.16550\tvalidation-rmse:0.21512\n",
      "[20]\ttrain-rmse:0.16494\tvalidation-rmse:0.21724\n",
      "[21]\ttrain-rmse:0.16462\tvalidation-rmse:0.21484\n",
      "[22]\ttrain-rmse:0.16411\tvalidation-rmse:0.21553\n",
      "[23]\ttrain-rmse:0.16430\tvalidation-rmse:0.21362\n",
      "[24]\ttrain-rmse:0.16371\tvalidation-rmse:0.21314\n",
      "[25]\ttrain-rmse:0.16338\tvalidation-rmse:0.21196\n",
      "[26]\ttrain-rmse:0.16256\tvalidation-rmse:0.20959\n",
      "[27]\ttrain-rmse:0.16128\tvalidation-rmse:0.20653\n",
      "[28]\ttrain-rmse:0.16150\tvalidation-rmse:0.20690\n",
      "[29]\ttrain-rmse:0.16073\tvalidation-rmse:0.20726\n",
      "[30]\ttrain-rmse:0.15989\tvalidation-rmse:0.20864\n",
      "[31]\ttrain-rmse:0.16010\tvalidation-rmse:0.20965\n",
      "[32]\ttrain-rmse:0.15975\tvalidation-rmse:0.20859\n",
      "[33]\ttrain-rmse:0.15945\tvalidation-rmse:0.20862\n",
      "[34]\ttrain-rmse:0.15914\tvalidation-rmse:0.20705\n",
      "[35]\ttrain-rmse:0.15899\tvalidation-rmse:0.20670\n",
      "[36]\ttrain-rmse:0.15858\tvalidation-rmse:0.20669\n",
      "[37]\ttrain-rmse:0.15807\tvalidation-rmse:0.20619\n",
      "[38]\ttrain-rmse:0.15779\tvalidation-rmse:0.20607\n",
      "[39]\ttrain-rmse:0.15796\tvalidation-rmse:0.20665\n",
      "[40]\ttrain-rmse:0.15786\tvalidation-rmse:0.20660\n",
      "[41]\ttrain-rmse:0.15715\tvalidation-rmse:0.20575\n",
      "[42]\ttrain-rmse:0.15609\tvalidation-rmse:0.20431\n",
      "[43]\ttrain-rmse:0.15616\tvalidation-rmse:0.20325\n",
      "[44]\ttrain-rmse:0.15579\tvalidation-rmse:0.20149\n",
      "[45]\ttrain-rmse:0.15531\tvalidation-rmse:0.20491\n",
      "[46]\ttrain-rmse:0.15588\tvalidation-rmse:0.20514\n",
      "[47]\ttrain-rmse:0.15598\tvalidation-rmse:0.20573\n",
      "[48]\ttrain-rmse:0.15546\tvalidation-rmse:0.20309\n",
      "[49]\ttrain-rmse:0.15625\tvalidation-rmse:0.20358\n",
      "[50]\ttrain-rmse:0.15593\tvalidation-rmse:0.20308\n",
      "[51]\ttrain-rmse:0.15613\tvalidation-rmse:0.20300\n",
      "[52]\ttrain-rmse:0.15544\tvalidation-rmse:0.20482\n",
      "[53]\ttrain-rmse:0.15553\tvalidation-rmse:0.20582\n",
      "[54]\ttrain-rmse:0.15462\tvalidation-rmse:0.20330\n"
     ]
    }
   ],
   "source": [
    "#now apply the new hyperparameters to the re-fit the model\n",
    "#note, eta = learning rate\n",
    "params_tuned = {\n",
    "    'objective': 'binary:logistic',  \n",
    "    'max_depth': 9,\n",
    "    'eta': 0.3,\n",
    "    'gamma': 0.3,\n",
    "    'subsample': 0.7,\n",
    "    'min_child_weight': 5,\n",
    "    'eval_metric': 'rmse',\n",
    "    'tree_method': 'hist'\n",
    "}\n",
    "\n",
    "evals = [(dtrain_reg, 'train'), (dtest_reg, 'validation')]\n",
    "\n",
    "model = xgb.train(\n",
    "    params_tuned,\n",
    "    dtrain_reg,\n",
    "    num_boost_round=200,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Observation</b>\n",
    "The training has early-stopped at less than 200 rounds. It is here that the training loss is low but also takes the slightly fluctuating validation loss into account which we want to watch to ensure we are not getting into overfitting with too many rounds.\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.36328\tvalidation-rmse:0.38229\n",
      "[50]\ttrain-rmse:0.02804\tvalidation-rmse:0.23320\n",
      "[100]\ttrain-rmse:0.01314\tvalidation-rmse:0.23394\n",
      "[150]\ttrain-rmse:0.00644\tvalidation-rmse:0.23437\n",
      "[200]\ttrain-rmse:0.00355\tvalidation-rmse:0.23438\n",
      "[250]\ttrain-rmse:0.00185\tvalidation-rmse:0.23444\n",
      "[300]\ttrain-rmse:0.00107\tvalidation-rmse:0.23445\n",
      "[350]\ttrain-rmse:0.00068\tvalidation-rmse:0.23449\n",
      "[400]\ttrain-rmse:0.00059\tvalidation-rmse:0.23450\n",
      "[450]\ttrain-rmse:0.00057\tvalidation-rmse:0.23449\n",
      "[500]\ttrain-rmse:0.00056\tvalidation-rmse:0.23449\n",
      "[550]\ttrain-rmse:0.00055\tvalidation-rmse:0.23450\n",
      "[600]\ttrain-rmse:0.00053\tvalidation-rmse:0.23450\n",
      "[650]\ttrain-rmse:0.00053\tvalidation-rmse:0.23450\n",
      "[700]\ttrain-rmse:0.00053\tvalidation-rmse:0.23449\n",
      "[750]\ttrain-rmse:0.00053\tvalidation-rmse:0.23449\n",
      "[800]\ttrain-rmse:0.00053\tvalidation-rmse:0.23449\n",
      "[850]\ttrain-rmse:0.00053\tvalidation-rmse:0.23449\n",
      "[900]\ttrain-rmse:0.00053\tvalidation-rmse:0.23449\n",
      "[950]\ttrain-rmse:0.00053\tvalidation-rmse:0.23449\n",
      "[999]\ttrain-rmse:0.00053\tvalidation-rmse:0.23449\n"
     ]
    }
   ],
   "source": [
    "# try a very high number of rounds to see if we can get a better RMSE for validation\n",
    "# if overfitting shows up we'll stick with the original model\n",
    "\n",
    "n_b = 1000\n",
    "\n",
    "model_b = xgb.train(\n",
    "    params_tuned, \n",
    "    dtrain_reg, \n",
    "    n_b, \n",
    "    evals=evals, \n",
    "    verbose_eval=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Observation</b>\n",
    "Interesting attempt to improve the model but as was shown in early-stop, there is little validation loss improvement even by the time the model reaches 50 rounds.\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 - Visualise the dataset and the model's results, where applicable like feature importance, confusion matrix, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 - Report the final performance of the selected model using the appropriate performance metrics like accuracy, F1-score, etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
